{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5977df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To drop unsupported openai params from the call, set `litellm.drop_params = True`\n",
    "# Example: ChatGPT O-series models don't support temperature=0.\n",
    "\n",
    "litellm.drop_params = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62ac22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = litellm.completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6656bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: bikashg.\n",
      "View Weave data at https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x12325d940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init(\"simple_prompting_guide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b38e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_FAST_MODEL_NAME = \"o4-mini\"\n",
    "OPENAI_SMART_MODEL_NAME = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1511650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def get_completion(system_message: str, user_messages: list, model: str, **kwargs):\n",
    "    # this fromat is specific to openai models only\n",
    "    formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + user_messages\n",
    "\n",
    "    # Common arguments for the litellm completion function\n",
    "    completion_args = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": kwargs.pop('max_tokens', 4096),\n",
    "        \"temperature\": kwargs.pop('temperature', 0),\n",
    "        \"messages\": formatted_messages\n",
    "    }\n",
    "    completion_args.update(kwargs)  # Include any other additional arguments; as such\n",
    "\n",
    "    # Generate and return the completion\n",
    "    response = completion(**completion_args)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d319485",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain the latest prompting techniques and provide an example of each.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ce77e",
   "metadata": {},
   "source": [
    "### Step 1: Raw Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e4f99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt_response = get_completion(\n",
    "    system_message= \"\",\n",
    "    user_messages = [{\"role\": \"user\", \"content\": question}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8baee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a rundown of some of the most effective recent prompting techniques for large language models (LLMs), with a brief definition and a concrete prompt example for each.\n",
      "\n",
      "1. Zero-Shot Prompting  \n",
      "  Definition: You give the model a task description (and no examples) and ask it to solve it directly.  \n",
      "  Example Prompt:  \n",
      "    “Translate the following sentence into French: ‘The cat sat on the mat.’”  \n",
      "\n",
      "2. Zero-Shot Chain-of-Thought (CoT)  \n",
      "  Definition: You ask the model to “think out loud” even  ... \n"
     ]
    }
   ],
   "source": [
    "print(raw_prompt_response[0:500] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1da82",
   "metadata": {},
   "source": [
    "### Step 2. Prompting with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aefd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The context we want to use here is the contents of the prompting guide file\n",
    "PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\"\n",
    "\n",
    "def load_markdown_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads and returns the content of a markdown file specified by its path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the markdown file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the markdown file as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_content = file.read()\n",
    "    return markdown_content\n",
    "context = load_markdown_file(PROMPT_GUIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e230586",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt_response = get_completion(\n",
    "    system_message= \"\",\n",
    "    user_messages = [{\"role\": \"user\", \"content\": context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93dc4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a concise overview of the most popular modern prompting techniques, with a one‐line description of each and a minimal illustrative prompt.\n",
      "\n",
      "1. Zero-Shot Prompting  \n",
      "   • Tell the model the task, no examples.  \n",
      "   • Example (translation):  \n",
      "     Prompt: “Translate to French: ‘How are you today?’”  \n",
      "     ⇒ Model: “Comment allez-vous aujourd’hui ?”\n",
      "\n",
      "2. Few-Shot Prompting  \n",
      "   • Prepend a handful of (input→output) examples to show style or format.  \n",
      "   • Example (sentiment classification):   ... \n"
     ]
    }
   ],
   "source": [
    "print(context_prompt_response[0:500] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e53a7",
   "metadata": {},
   "source": [
    "### Step 3. Condition Responses with a System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66320bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "Objective: Simplify prompt engineering concepts for easy understanding. Provide clear examples for each technique.\n",
    "Tone: Friendly and educational, suitable for beginners.\n",
    "Context: Assume basic AI knowledge; avoid deep technical jargon.\n",
    "Guidance: Use metaphors and simple examples to explain concepts. Keep explanations concise and applicable.\n",
    "Verification: Ensure clarity and relevance in responses, with practical examples.\n",
    "Benefits: Help users grasp prompt engineering basics, enhancing their AI interaction experience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a977505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_and_context_prompt_response = get_completion(\n",
    "    system_message= system_message,\n",
    "    user_messages = [{\"role\": \"user\", \"content\": context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e6a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a friendly, “for-beginners” tour of today’s most popular prompt-engineering tricks. Think of each as a little recipe—first I’ll tell you what it does in plain English (with a quick metaphor), then I’ll show you a super-simple example.  \n",
      "\n",
      "1. Zero-Shot Prompting  \n",
      "  • What it is: You walk up to the model cold and ask your question—no examples, no prep.  \n",
      "  • Metaphor: Like quizzing someone on the spot with “What’s 7 × 8?”  \n",
      "  • Example:  \n",
      "    “Translate ‘Good morning’ into French.”  \n",
      "\n",
      "2. Fe ... \n"
     ]
    }
   ],
   "source": [
    "print(system_and_context_prompt_response[0:500] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6d27b",
   "metadata": {},
   "source": [
    "### Step 4: System Prompts - Inputs. \n",
    "\n",
    "### Use f formatted strings to include placeholders in user message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b5d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_prompt_template = \"{input_context}\\n\\n{input_question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15a334c2-068c-4632-982b-184656242daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable tracking of context and questions dynamically fed at runtime, we will write a function and decorate it with @weave.op\n",
    "@weave.op()\n",
    "def format_prompt(template_string: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Formats a prompt template with provided keyword arguments.\n",
    "\n",
    "    This function takes a template string and a dictionary of keyword arguments,\n",
    "    then formats the template string using these arguments.\n",
    "\n",
    "    Parameters:\n",
    "        prompt_template (str): The template string to be formatted.\n",
    "        **kwargs (dict): Keyword arguments to format the template string with.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt template.\n",
    "    \"\"\"\n",
    "    return template_string.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bd9758a-d493-4683-b286-97e8ae014926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test context\\ntest question'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a simple check\n",
    "format_prompt(template_string=user_message_prompt_template, input_context=\"test context\", input_question=\"test question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6906517-7740-422a-84bb-da590724cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_instantiated_user_message = format_prompt(template_string=user_message_prompt_template, input_context=context, input_question=\"\"\"\n",
    "Explain the differences between zero-shot, few-shot, and chain of thought \n",
    "prompting techniques? Please provide a clear explanation and a practical example \n",
    "for each technique within a structured format.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dbe64f4-51c6-49c1-939b-ae1bfce72821",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_and_template_instantiated_user_message_response = get_completion(\n",
    "    system_message= system_message,\n",
    "    user_messages = [{\"role\": \"user\", \"content\": template_instantiated_user_message}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce3f5f-6c6f-4c6e-b1fb-a1d2e23ca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_and_template_instantiated_user_message_response[0:500] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eec4b4-6aa7-4a04-b6dc-a3237b4a37d3",
   "metadata": {},
   "source": [
    "### Step 5: System Prompts - Outputs\n",
    "\n",
    "In this step, we focus on improving the consistency and structure of our model's outputs by modifying the prompt template. By including specific tags and formatting instructions in the prompt, we can guide the model to respond in a way that is easier to parse and process. Get the ouput in XML, JSON etc. GPT prefers JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f2e9f02-c3b1-4a3b-9ec0-968f5da33751",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_formatting_instruction = \"\"\"\n",
    "Format: Respond within a structured JSON object, using the keys provided in the prompt to organize your response.\n",
    "Provide a condensed answer under the 'condensed_answer' key, detailed explanations under 'explanation' keys, \n",
    "and examples under 'example' keys within each explanation.\n",
    "\"\"\"\n",
    "\n",
    "user_msg_formatting_instruction = \"\"\"\n",
    "You must respond in JSON format.\n",
    "Your response should follow this structure:\n",
    "{{ \n",
    "  \"answer\": {{\n",
    "    \"condensed_answer\": \"CONDENSED_ANSWER\",\n",
    "    \"explanation_1\": {{\n",
    "      \"detail\": \"EXPLANATION_1\",\n",
    "      \"example\": \"EXAMPLE_1\"\n",
    "    }},\n",
    "    \"explanation_2\": {{\n",
    "      \"detail\": \"EXPLANATION_2\",\n",
    "      \"example\": \"EXAMPLE_2\"\n",
    "    }},\n",
    "    ...\n",
    "  }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0308a66-4337-434a-aa45-fc370bcbeefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_prompt_with_output_formatting_instructions(in_system_message: str, in_user_message: str):\n",
    "    return in_system_message + \"\\n\" + system_msg_formatting_instruction, in_user_message + \"\\n\" + user_msg_formatting_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97829e37-b602-4173-b48b-262e02d3a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_formatted_system_message, output_formatted_user_message = get_updated_prompt_with_output_formatting_instructions(system_message, template_instantiated_user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69889b2b-2963-40f0-a62f-70b364e9d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_formatted_system_and_user_message_response = get_completion(\n",
    "    system_message= output_formatted_system_message,\n",
    "    user_messages = [{\"role\": \"user\", \"content\": output_formatted_user_message}],\n",
    "    model = OPENAI_FAST_MODEL_NAME,\n",
    "    response_format={\"type\": \"json_object\"}  # An additional thing here is that we need to specify this.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef5c2b35-5918-4207-aa6f-5d908ffdddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":{\"condensed_answer\":\"Zero-shot asks the model to perform a task with no examples, few-shot provides a handful of demonstrations before the query, and chain-of-thought adds step-by-step reasoning to guide the answer.\",\"explanation_1\":{\"detail\":\"Zero-shot prompting is like asking a friend a question without giving any hints or examples. You simply state the task and rely on the model’s existing knowledge.\",\"example\":\"Prompt: What is the capital of Germany?\\\\nAnswer:\"},\"explanation_2\":{\"d ... \n"
     ]
    }
   ],
   "source": [
    "print(output_formatted_system_and_user_message_response[0:500] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141aebac-3ab2-4c76-afd5-e258d0853793",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4044f-5724-4f4f-bd8b-22a31806683b",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "277e8df4-20a8-4fd1-af39-8dec5eddf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the above are examples of zero shot prompting. We didn't show any examples of any output (tone, style, etc.) we would like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a5512-5916-44a9-9161-8ce0bfb7eb37",
   "metadata": {},
   "source": [
    "### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfbd08-d494-4934-8f9f-046e239fa56e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
