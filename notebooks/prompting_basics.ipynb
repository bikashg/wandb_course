{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5977df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To drop unsupported openai params from the call, set `litellm.drop_params = True`\n",
    "# Example: ChatGPT O-series models don't support temperature=0.\n",
    "\n",
    "litellm.drop_params = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62ac22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = litellm.completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6656bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: bikashg.\n",
      "View Weave data at https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x11885d940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init(\"simple_prompting_guide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b38e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_FAST_MODEL_NAME = \"o4-mini\"\n",
    "OPENAI_SMART_MODEL_NAME = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1511650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def get_completion(system_message: str, user_messages: list, model: str, **kwargs):\n",
    "    # this fromat is specific to openai models only\n",
    "    formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + user_messages\n",
    "\n",
    "    # Common arguments for the litellm completion function\n",
    "    completion_args = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": kwargs.pop('max_tokens', 4096),\n",
    "        \"temperature\": kwargs.pop('temperature', 0),\n",
    "        \"messages\": formatted_messages\n",
    "    }\n",
    "    completion_args.update(kwargs)  # Include any other additional arguments; as such\n",
    "\n",
    "    # Generate and return the completion\n",
    "    response = completion(**completion_args)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d319485",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain the latest prompting techniques and provide an example of each.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ce77e",
   "metadata": {},
   "source": [
    "### Step 1: Raw Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4f99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c58-838e-7183-bb9a-de055d68d95e\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c58-d5fc-7231-819a-ded332d49179\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c59-0f92-7612-b243-c57bcbcee6d0\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c59-51a5-7980-9f5a-4839c4438bc1\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c59-5195-7f01-a42d-88b23e3ae7eb\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c59-51a1-70a3-a42f-4d701680d410\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c59-8493-7460-8c81-4b756f0b9b43\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c5f-545d-7aa3-838b-f26dd419b4e4\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c76-40c8-78d1-a442-e11ebac5032e\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c77-a693-7700-8948-ab4f36fde05c\n",
      "üç© https://wandb.ai/bikashg-spencer-group/simple_prompting_guide/r/call/01968c78-4e2f-7563-8698-e48110bcc30e\n"
     ]
    }
   ],
   "source": [
    "raw_prompt_response = get_completion(\n",
    "    system_message= \"\",\n",
    "    user_messages = [{\"role\": \"user\", \"content\": question}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8baee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here‚Äôs a survey of nine of the most-talked-about ‚Äúnext-gen‚Äù prompting patterns‚Äîwhat they are, why they help, and a minimal example for each.  \n",
      "\n",
      "1. Zero-Shot Chain-of-Thought (CoT)  \n",
      "  ‚Ä¢ Idea: Ask the model to ‚Äúthink step by step‚Äù even with no demonstrations.  \n",
      "  ‚Ä¢ Why it helps: Elicits internal reasoning, reducing rash one-sentence answers.  \n",
      "  ‚Ä¢ Prompt:  \n",
      "    Q: ‚ÄúIf there are 12 apples and I give half to a friend, how many are left?  \n",
      "       Let‚Äôs think step by step.‚Äù  \n",
      "  ‚Ä¢ Model output (abridg ... \n"
     ]
    }
   ],
   "source": [
    "print(raw_prompt_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1da82",
   "metadata": {},
   "source": [
    "### Step 2. Prompting with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aefd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The context we want to use here is the contents of the prompting guide file\n",
    "PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\"\n",
    "\n",
    "def load_markdown_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads and returns the content of a markdown file specified by its path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the markdown file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the markdown file as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_content = file.read()\n",
    "    return markdown_content\n",
    "context = load_markdown_file(PROMPT_GUIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e230586",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt_response = get_completion(\n",
    "    system_message= \"\",\n",
    "    user_messages = [{\"role\": \"user\", \"content\": context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93dc4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a concise tour of the most widely‚Äêused modern prompting techniques, each with a minimal description and a toy prompt example. Feel free to adapt them to your own task.\n",
      "\n",
      "1. Zero‚ÄêShot Prompting  \n",
      "   ‚Ä¢ Description: You describe the task and ask the model directly, with no demonstrations.  \n",
      "   ‚Ä¢ Example:  \n",
      "     ```\n",
      "     Translate the following sentence into French.\n",
      "     Input: ‚ÄúHow are you today?‚Äù\n",
      "     Output:\n",
      "     ```\n",
      "\n",
      "2. Few‚ÄêShot Prompting  \n",
      "   ‚Ä¢ Description: You prepend a handful of (inp ... \n"
     ]
    }
   ],
   "source": [
    "print(context_prompt_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e53a7",
   "metadata": {},
   "source": [
    "### Step 3. Condition Responses with a System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66320bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "Objective: Simplify prompt engineering concepts for easy understanding. Provide clear examples for each technique.\n",
    "Tone: Friendly and educational, suitable for beginners.\n",
    "Context: Assume basic AI knowledge; avoid deep technical jargon.\n",
    "Guidance: Use metaphors and simple examples to explain concepts. Keep explanations concise and applicable.\n",
    "Verification: Ensure clarity and relevance in responses, with practical examples.\n",
    "Benefits: Help users grasp prompt engineering basics, enhancing their AI interaction experience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a977505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_and_context_prompt_response = get_completion(\n",
    "    system_message= system_message,\n",
    "    user_messages = [{\"role\": \"user\", \"content\": context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e6a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here‚Äôs a friendly, beginner-level tour of the main prompt-engineering tricks, each with a simple ‚Äúreal-world‚Äù metaphor and a tiny example you can try yourself.\n",
      "\n",
      "1. Basic Prompting  \n",
      "   ‚Ä¢ Zero-Shot (‚ÄúAsking cold‚Äù)  \n",
      "     Metaphor: You walk up to a friend and just ask, ‚ÄúWhy is the sky blue?‚Äù  \n",
      "     Example:  \n",
      "       Prompt: ‚ÄúExplain why the sky is blue.‚Äù  \n",
      "       Model: ‚ÄúSunlight scatters in the atmosphere‚Ä¶.‚Äù  \n",
      "   ‚Ä¢ Few-Shot (‚ÄúShowing examples‚Äù)  \n",
      "     Metaphor: You show two photos of cats and dog ... \n"
     ]
    }
   ],
   "source": [
    "print(system_and_context_prompt_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6d27b",
   "metadata": {},
   "source": [
    "### Step 4: System Prompts - Inputs. \n",
    "\n",
    "### Use f formatted strings to include placeholders in user message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b5d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_prompt_template = \"{input_context}\\n\\n{input_question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15a334c2-068c-4632-982b-184656242daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable tracking of context and questions dynamically fed at runtime, we will write a function and decorate it with @weave.op\n",
    "@weave.op()\n",
    "def format_prompt(template_string: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Formats a prompt template with provided keyword arguments.\n",
    "\n",
    "    This function takes a template string and a dictionary of keyword arguments,\n",
    "    then formats the template string using these arguments.\n",
    "\n",
    "    Parameters:\n",
    "        prompt_template (str): The template string to be formatted.\n",
    "        **kwargs (dict): Keyword arguments to format the template string with.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt template.\n",
    "    \"\"\"\n",
    "    return template_string.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bd9758a-d493-4683-b286-97e8ae014926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test context\\n\\ntest question'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a simple check\n",
    "format_prompt(template_string=user_message_prompt_template, input_context=\"test context\", input_question=\"test question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6906517-7740-422a-84bb-da590724cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_instantiated_user_message = format_prompt(template_string=user_message_prompt_template, input_context=context, input_question=\"\"\"\n",
    "Explain the differences between zero-shot, few-shot, and chain of thought \n",
    "prompting techniques? Please provide a clear explanation and a practical example \n",
    "for each technique within a structured format.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dbe64f4-51c6-49c1-939b-ae1bfce72821",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_and_template_instantiated_user_message_response = get_completion(\n",
    "    system_message= system_message,\n",
    "    user_messages = [{\"role\": \"user\", \"content\": template_instantiated_user_message}],\n",
    "    model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cce3f5f-6c6f-4c6e-b1fb-a1d2e23ca405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here‚Äôs a simple breakdown of three core prompting styles‚Äîzero-shot, few-shot, and chain-of-thought‚Äîeach with a friendly metaphor, a plain-English explanation, and a hands-on example.\n",
      "\n",
      "1. Zero-Shot Prompting  \n",
      "   ‚Ä¢ Metaphor: Asking a friend a question ‚Äúcold,‚Äù with no hints.  \n",
      "   ‚Ä¢ What it is: You give the model just your request (the ‚Äútask‚Äù) and let it answer directly. You don‚Äôt supply any examples of how to do it first.  \n",
      "   ‚Ä¢ Why it helps: Super quick‚Äîuses few tokens‚Äîand you don‚Äôt need to prepa ... \n"
     ]
    }
   ],
   "source": [
    "print(system_and_template_instantiated_user_message_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eec4b4-6aa7-4a04-b6dc-a3237b4a37d3",
   "metadata": {},
   "source": [
    "### Step 5: System Prompts - Outputs\n",
    "\n",
    "In this step, we focus on improving the consistency and structure of our model's outputs by modifying the prompt template. By including specific tags and formatting instructions in the prompt, we can guide the model to respond in a way that is easier to parse and process. Get the ouput in XML, JSON etc. GPT prefers JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f2e9f02-c3b1-4a3b-9ec0-968f5da33751",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_formatting_instruction = \"\"\"\n",
    "Format: Respond within a structured JSON object, using the keys provided in the prompt to organize your response.\n",
    "Provide a condensed answer under the 'condensed_answer' key, detailed explanations under 'explanation' keys, \n",
    "and examples under 'example' keys within each explanation.\n",
    "\"\"\"\n",
    "\n",
    "user_msg_formatting_instruction = \"\"\"\n",
    "You must respond in JSON format.\n",
    "Your response should follow this structure:\n",
    "{{ \n",
    "  \"answer\": {{\n",
    "    \"condensed_answer\": \"CONDENSED_ANSWER\",\n",
    "    \"explanation_1\": {{\n",
    "      \"detail\": \"EXPLANATION_1\",\n",
    "      \"example\": \"EXAMPLE_1\"\n",
    "    }},\n",
    "    \"explanation_2\": {{\n",
    "      \"detail\": \"EXPLANATION_2\",\n",
    "      \"example\": \"EXAMPLE_2\"\n",
    "    }},\n",
    "    ...\n",
    "  }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0308a66-4337-434a-aa45-fc370bcbeefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_prompt_with_output_formatting_instructions(in_system_message: str, in_user_message: str):\n",
    "    return in_system_message + \"\\n\" + system_msg_formatting_instruction, in_user_message + \"\\n\" + user_msg_formatting_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97829e37-b602-4173-b48b-262e02d3a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_formatted_system_message, output_formatted_user_message = get_updated_prompt_with_output_formatting_instructions(system_message, template_instantiated_user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69889b2b-2963-40f0-a62f-70b364e9d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_formatted_system_and_user_message_response = get_completion(\n",
    "    system_message= output_formatted_system_message,\n",
    "    user_messages = [{\"role\": \"user\", \"content\": output_formatted_user_message}],\n",
    "    model = OPENAI_FAST_MODEL_NAME,\n",
    "    response_format={\"type\": \"json_object\"}  # An additional thing here is that we need to specify this.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef5c2b35-5918-4207-aa6f-5d908ffdddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":{\"condensed_answer\":\"Zero-shot: ask the model directly without examples. Few-shot: show a few examples to guide the model. Chain-of-Thought: prompt the model to explain its reasoning step by step before answering.\",\"explanation_1\":{\"detail\":\"Zero-shot prompting is like asking a friend a question without any context or samples. You state the task clearly and trust the model‚Äôs pre-trained knowledge to answer.\",\"example\":\"Example: Translate to French:\\n\\\"The weather is nice today.\\\"\"},\"ex ... \n"
     ]
    }
   ],
   "source": [
    "print(output_formatted_system_and_user_message_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141aebac-3ab2-4c76-afd5-e258d0853793",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4044f-5724-4f4f-bd8b-22a31806683b",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277e8df4-20a8-4fd1-af39-8dec5eddf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the above are examples of zero shot prompting. We didn't show any examples of any output (tone, style, etc.) we would like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a5512-5916-44a9-9161-8ce0bfb7eb37",
   "metadata": {},
   "source": [
    "### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82bfbd08-d494-4934-8f9f-046e239fa56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = \"\"\"\n",
    "Here are a few examples of prompting techniques in JSON format:\n",
    "{{\n",
    "    \"answer\": {{\n",
    "        \"condensed_answer\": \"Different prompting techniques are used to guide language models in generating desired outputs.\",\n",
    "        \"explanation_1\": {{\n",
    "            \"detail\": \"Translation prompts provide the model with a source language text and request the translation in a target language.\",\n",
    "            \"example\": \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "        }},\n",
    "        \"explanation_2\": {{\n",
    "            \"detail\": \"Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.\",\n",
    "            \"example\": \"Classify the sentiment of the following text: 'The movie was terrible.'\"\n",
    "        }},\n",
    "        \"explanation_3\": {{\n",
    "            \"detail\": \"Factual question prompts require the model to provide an answer along with an explanation or reasoning.\",\n",
    "            \"example\": \"What is the capital of Germany? Explain your reasoning.\"\n",
    "        }}\n",
    "    }}\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c37b0b97-4338-468a-9f02-87eb288dfc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_user_message = few_shot_examples + \"\\n\" + output_formatted_user_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33050560-5954-4adf-bf9d-541e05ed8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(few_shot_user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adb1142b-718b-4909-8e2d-f9f608f2f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_response = get_completion(\n",
    "    system_message=output_formatted_system_message,\n",
    "    user_messages=[{\"role\": \"user\", \"content\": few_shot_user_message}],\n",
    "     model = OPENAI_FAST_MODEL_NAME,\n",
    "    response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "beed7027-d4a5-4583-9ffc-72d73fd937f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": {\n",
      "    \"condensed_answer\": \"Zero-shot asks directly with no examples, few-shot shows a handful of examples to guide the model, and chain-of-thought asks the model to explain its reasoning step by step before answering.\",\n",
      "    \"explanation_1\": {\n",
      "      \"detail\": \"Zero-shot prompting gives the model a task instruction without any demonstrations. Imagine asking someone to solve a puzzle without showing any solved examples first.\",\n",
      "      \"example\": \"Prompt: \\\"How many pencils do you have  ... \n"
     ]
    }
   ],
   "source": [
    "print(few_shot_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8895f-7a73-4353-b55c-ed54e406a430",
   "metadata": {},
   "source": [
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df645397-510a-4b52-8778-9c6d452d8349",
   "metadata": {},
   "source": [
    "Note: We do not use the output formatting in this case as it will negate the chain of thought to instead enforce the formatting. It is important to explicitly incorporate the thought process desired in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b9e6958-d7ef-468c-b99d-1ef32aa31188",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought_system_format = \"Format: You must explicitly define the thought process and knowledge from the context to come to your conclusion for the question.\"\n",
    "chain_of_thought_instruction = \"Let's explicitly think step by step. My thought process is:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a889babb-ac0a-454e-a1d3-83d0806f4a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objective: Simplify prompt engineering concepts for easy understanding. Provide clear examples for each technique.\n",
      "Tone: Friendly and educational, suitable for beginners.\n",
      "Context: Assume basic AI knowledge; avoid deep technical jargon.\n",
      "Guidance: Use metaphors and simple examples to explain concepts. Keep explanations concise and applicable.\n",
      "Verification: Ensure clarity and relevance in responses, with practical examples.\n",
      "Benefits: Help users grasp prompt engineering basics, enhancing their AI interaction experience.\n",
      "\n",
      "Format: You must explicitly define the thought process and knowledge from the context to come to your conclusion for the question.\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought_system_message = system_message + \"\\n\" + chain_of_thought_system_format\n",
    "print(chain_of_thought_system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4904dd6-2201-42e3-83e6-2b92a3916949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... t support tool use in a chain (i.e. using the output of one tool as an input for another tool) or in an interactive way (i.e. adopt API response after human selection). Both are interesting future directions to expand the model for.\n",
      "\n",
      "\n",
      "Explain the differences between zero-shot, few-shot, and chain of thought \n",
      "prompting techniques? Please provide a clear explanation and a practical example \n",
      "for each technique within a structured format.\n",
      "\n",
      "Let's explicitly think step by step. My thought process is:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought_user_message = template_instantiated_user_message + \"\\n\" + chain_of_thought_instruction\n",
    "print(\" ... \"+ chain_of_thought_user_message[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc73f00a-c9f5-48bd-a606-a96ccea792ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought_response = get_completion(\n",
    "    system_message=chain_of_thought_system_message,\n",
    "    user_messages=[{\"role\": \"user\", \"content\": chain_of_thought_user_message}],\n",
    "     model = OPENAI_FAST_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce93bc69-ab59-43ae-ae22-9b3f09056439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought Process (concise summary):\n",
      "- I know from the context that zero-shot means ‚Äúno examples,‚Äù few-shot means ‚Äúsome examples,‚Äù and chain-of-thought (CoT) means ‚Äúask the model to show its reasoning steps.‚Äù\n",
      "- I‚Äôll pick a simple math problem (‚Äú2+2‚Äù) to illustrate all three, because arithmetic is familiar and keeps the focus on the prompting style rather than domain details.\n",
      "- I‚Äôll structure each technique into: Definition, Metaphor, and Practical Example.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. Zero-Shot Prompting  \n",
      "   ‚Ä¢ D ... \n"
     ]
    }
   ],
   "source": [
    "print(chain_of_thought_response[0:1000] + \" ... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def60a22-67d0-4b26-a640-eaeae30c15d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
